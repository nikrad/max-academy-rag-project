{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The strategic plan covers the years 2024-2028.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load documents from a directory (you can change this path as needed)\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# Create an index from the documents\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Example query\n",
    "response = query_engine.query(\"What years does the strategic plan cover?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant documents: 2\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document 1:\n",
      "Text sample: CodePath\n",
      "2024-28\n",
      "Strategic\n",
      "Plan\n",
      "+\n",
      "Appendices\n",
      "(V1\n",
      "|\n",
      "04.23.24)\n",
      "Executive\n",
      "Summary\n",
      "Problem\n",
      "Solution\n",
      "Impact\n",
      "and\n",
      "Evidence\n",
      "To\n",
      "Date\n",
      "Introduction\n",
      "to\n",
      "the\n",
      "2024-28\n",
      "Plan\n",
      "Pillar\n",
      "1:\n",
      "Scale\n",
      "Nationally\n",
      "in\n",
      "Breadth,\n",
      "and\n",
      "...\n",
      "Metadata: {'page_label': '1', 'file_name': 'CodePath strategic plan.pdf', 'file_path': '/Users/nikrad/MaxAcademy/wk_2_rag_demo/data/CodePath strategic plan.pdf', 'file_type': 'application/pdf', 'file_size': 1452743, 'creation_date': '2024-09-17', 'last_modified_date': '2024-09-16'}\n",
      "Score: 0.8278842030229911\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document 2:\n",
      "Text sample: 2023-24\n",
      "Corporate\n",
      "Partners\n",
      "Alteryx\n",
      "Amazon\n",
      "Asurion\n",
      "Base10\n",
      "Bentley\n",
      "Systems\n",
      "Bill.com\n",
      "Bloomberg\n",
      "Boeing\n",
      "Brooks\n",
      "Running\n",
      "Course\n",
      "Hero\n",
      "Dickâ€™s\n",
      "Sporting\n",
      "Goods\n",
      "Docusign\n",
      "Facebook\n",
      "Fastly\n",
      "Lyft\n",
      "Meta\n",
      "Microsoft\n",
      "Mutual\n",
      "...\n",
      "Metadata: {'page_label': '48', 'file_name': 'CodePath strategic plan.pdf', 'file_path': '/Users/nikrad/MaxAcademy/wk_2_rag_demo/data/CodePath strategic plan.pdf', 'file_type': 'application/pdf', 'file_size': 1452743, 'creation_date': '2024-09-17', 'last_modified_date': '2024-09-16'}\n",
      "Score: 0.8113748697440457\n",
      "\n",
      "==================================================\n",
      "\n",
      "LLM Response:\n",
      "The strategic plan covers the years 2024-2028.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Create an index from the documents\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a retriever to fetch relevant documents\n",
    "retriever = index.as_retriever(retrieval_mode='similarity', k=3)\n",
    "\n",
    "# Define your query\n",
    "query = \"What years does the strategic plan cover?\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "relevant_docs = retriever.retrieve(query)\n",
    "\n",
    "print(f\"Number of relevant documents: {len(relevant_docs)}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Text sample: {doc.node.get_content()[:200]}...\")  # Print first 200 characters\n",
    "    print(f\"Metadata: {doc.node.metadata}\")\n",
    "    print(f\"Score: {doc.score}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "endpoint_url = \"https://api.openai.com/v1\"\n",
    "client = openai.AsyncClient(api_key=api_key, base_url=endpoint_url)\n",
    "\n",
    "\n",
    "# Craft an LLM prompt that combines the documents with the query\n",
    "prompt = f\"\"\"Based on the following documents, please answer the query: \"{query}\"\n",
    "\n",
    "Relevant documents:\n",
    "\"\"\"\n",
    "\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    prompt += f\"\\nDocument {i+1}:\\n{doc.node.get_content()}\\n\"\n",
    "\n",
    "prompt += \"\\nPlease provide a short answer based on the information in these documents.\"\n",
    "\n",
    "# Generate a response using the OpenAI API\n",
    "async def generate_response(prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"chatgpt-4o-latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided documents.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run the async function\n",
    "llm_response = await generate_response(prompt)\n",
    "\n",
    "print(\"LLM Response:\")\n",
    "print(llm_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
